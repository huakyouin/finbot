{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_Dataset_of_financial_news_sentiment_classification(from_dir, to_dir):\n",
    "    # https://github.com/wwwxmu/Dataset-of-financial-news-sentiment-classification\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    train_data = pd.read_csv(f\"{from_dir}/train_data.csv\")\n",
    "    test_data = pd.read_csv(f\"{from_dir}/test_data.csv\")\n",
    "    get_text = lambda x: x['正文']\n",
    "    get_label = lambda x: x['正负面']\n",
    "    # 合并数据集\n",
    "    df = pd.concat([\n",
    "        train_data.assign(split='train', text=get_text, label=get_label),\n",
    "        test_data.assign(split='test', text=get_text, label=get_label)\n",
    "    ], ignore_index=True)[['split', 'text', 'label']]\n",
    "    # 清洗\n",
    "    df = df[df['text'].notna() & (df['text'] != '')]\n",
    "    # 保存清洗后的数据\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df.to_json(os.path.join(to_dir,'Dataset-of-financial-news-sentiment-classification.jsonl'), orient='records',force_ascii=False, lines=True)\n",
    "\n",
    "\n",
    "def custom_qlib_stock_dataset(from_dir, to_dir):\n",
    "    # https://github.com/chenditc/investment_data\n",
    "    import qlib\n",
    "    import os\n",
    "    from qlib.data import D\n",
    "    # 初始化 Qlib 的数据存储\n",
    "    qlib.init(provider_uri = from_dir)\n",
    "    fields = ['$open', '$high', '$low', '$close', '$volume', '$amount', '$vwap']\n",
    "    df = D.features(D.instruments(market='csi300'), fields, start_time='20160101', end_time='20201231', freq='day')\n",
    "    df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df.to_csv(os.path.join(to_dir,\"csi300_stock_feats.csv\"))\n",
    "\n",
    "\n",
    "def custom_FinCUGE(from_dir, to_dir):\n",
    "    # https://huggingface.co/datasets/Maciel/FinCUGE-Instruction\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    import os\n",
    "    dataset = load_dataset(from_dir)  \n",
    "    for split_name in dataset:\n",
    "        dataset[split_name] = dataset[split_name].map(lambda example: {\"split\": split_name})\n",
    "    combined_data = concatenate_datasets([dataset[split_name] for split_name in dataset])\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df = combined_data.to_pandas()\n",
    "    df.to_json(os.path.join(to_dir,\"FinCUGE.jsonl\"), orient='records',force_ascii=False, lines=True)\n",
    "\n",
    "\n",
    "def custom_CPTS(from_dir, to_dir):\n",
    "    # https://github.com/fjiangAI/CPTS\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    def boundary_to_mass(boundary_list):\n",
    "        bound_idx = [-1] + [i for i, val in enumerate(boundary_list) if val==1]\n",
    "        return [ bound_idx[i+1] - bound_idx[i] for i in range(len(bound_idx) - 1) ]\n",
    "    train_data = pd.read_json(f\"{from_dir}/train.json\")\n",
    "    test_data = pd.read_json(f\"{from_dir}/test.json\")\n",
    "    df = pd.concat([train_data.assign(split='train'),test_data.assign(split='test')], ignore_index=True) # 合并数据集\n",
    "    df['sentences'] = df.apply(lambda row: [i['text'] for i in row['paragraph_list']], axis=1)\n",
    "    df['masses'] = df['label_list'].apply(boundary_to_mass)\n",
    "    # 保存\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df[['id','title','topic_list','sentences', 'masses','split','label_list']].to_json(os.path.join(to_dir,'CPTS.jsonl'), orient='records',force_ascii=False, lines=True)\n",
    "\n",
    "def extract_finna_in_FinCUGE_for_sft(from_dir, to_dir):\n",
    "    import pandas as pd  \n",
    "    import os\n",
    "    df = pd.read_json(f\"{from_dir}/FinCUGE.jsonl\", lines=True)\n",
    "    df['messages'] = df.apply(lambda row: [\n",
    "                                dict(role='system',content=row['instruction']),\n",
    "                                dict(role='user',content=row['input']),\n",
    "                                dict(role='assistant',content=row['output'])\n",
    "                            ], axis=1)\n",
    "    df[(df['split'] == 'train') & (df['task'] == 'FINNA')][['messages']].to_json(os.path.join(to_dir,\"FinCUGE_FINNA_train.jsonl\"), orient='records',force_ascii=False, lines=True)\n",
    "    df[(df['split'] == 'eval') & (df['task'] == 'FINNA')][['messages']].to_json(os.path.join(to_dir,\"FinCUGE_FINNA_eval.jsonl\"), orient='records',force_ascii=False, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_dir = \"../resources/data/raw/CPTS\"\n",
    "to_dir = \"../resources/data/cleaned\"\n",
    "custom_CPTS(from_dir,to_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
