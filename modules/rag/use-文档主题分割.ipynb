{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config\n",
    "ebd_model_path = '../../resources/open_models/bge-large-zh-v1.5'\n",
    "seq_model_path = '../../resources/open_models/nlp_bert_document-segmentation_chinese-base'\n",
    "method = \"doc_seg_model_spliter\"\n",
    "FROM_DIR = '../../resources/data/_raw/CSI300news'\n",
    "TO_DIR = '../../resources/data/CSI300news_chunked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys; sys.path.append(\"../..\")\n",
    "from utils.spliters import *\n",
    "\n",
    "def init_spliter(method):\n",
    "    if method == \"cos_sim_spliter\":\n",
    "        from FlagEmbedding import FlagModel\n",
    "        model = FlagModel(ebd_model_path, query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\", use_fp16=True)\n",
    "        spliter = BaseSpliter.use_subclass(\"cos_sim_spliter\")(model)\n",
    "\n",
    "    elif method == \"doc_seq_model_spliter\":\n",
    "        from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "        model = AutoModelForTokenClassification.from_pretrained(seq_model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(seq_model_path)\n",
    "        spliter = BaseSpliter.use_subclass(\"doc_seq_model_spliter\")(model, tokenizer)\n",
    "    return spliter\n",
    "\n",
    "def process_stock_news(from_dir, to_dir, spliter):\n",
    "    os.makedirs(to_dir,exist_ok=True)\n",
    "    done_files = os.listdir(to_dir)\n",
    "    for filename in os.listdir(from_dir):\n",
    "        stock_id = filename.split('.')[0]\n",
    "        if not filename.endswith(\".csv\") or stock_id+\".json\" in done_files: continue\n",
    "        print(f\"当前id：{stock_id}\")\n",
    "        filepath = os.path.join(from_dir, filename)\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(filepath)\n",
    "        # 初始化一个空的DataFrame来存储结果\n",
    "        result = []\n",
    "        # 处理每行数据\n",
    "        for index, row in tqdm(df.iterrows()):\n",
    "                # 假设文件名格式为\"000001.csv\"\n",
    "            date = row['Date']\n",
    "            title = row['Title']\n",
    "            content = row['Content']\n",
    "            sentence_df = spliter.split_text_to_sentences(content)\n",
    "            sentence_df = spliter.add_buffered_sentences(sentence_df)\n",
    "            chunk_df = spliter.cluster(sentence_df)\n",
    "            # 将结果合并到原始DataFrame中\n",
    "            for _, sentence_row in chunk_df.iterrows():\n",
    "                result.append({\n",
    "                    'stock_id': stock_id,\n",
    "                    'date': date,\n",
    "                    'title': title,\n",
    "                    'content': content,\n",
    "                    'chunk': sentence_row['chunk'],\n",
    "                    'start_idx': sentence_row['start_idx'],\n",
    "                    'end_idx': sentence_row['end_idx']\n",
    "                })\n",
    "        print(f\"stock_id {stock_id} chunked.\")  # 打印或保存结果DataFrame\n",
    "        # break\n",
    "        pd.DataFrame(result).to_json(os.path.join(to_dir,f'{stock_id}.json'), force_ascii=False, orient='records', indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用函数处理文件夹中的所有文件\n",
    "spliter = init_spliter(method=method)\n",
    "process_stock_news(FROM_DIR, TO_DIR, spliter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
