{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newdisk/jxh/anaconda/envs/sft2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "/home/jxh/.cache/huggingface/modules/transformers_modules/Qwen-7B-Chat/modeling_qwen.py:314: UserWarning: KV cache kernel source files (.cpp and .cu) not found.\n",
      "  warnings.warn(\"KV cache kernel source files (.cpp and .cu) not found.\")\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,2\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import  AutoTokenizer,AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_id = \"models/Base/Qwen-7B-Chat\"\n",
    "model_id = \"models/Lora/Qwen_7B_Chat_lora_step1\"\n",
    "model_id = \"models/Lora/Qwen_7B_Chat_lora_step2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    load_in_4bit = True,\n",
    "    use_cache_quantization=True,\n",
    "    use_cache_kernel=True, \n",
    "    trust_remote_code=True,\n",
    "    use_flash_attn = False,\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False, \n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_adapters()\n",
    "# model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"chat_format\": \"chatml\",\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"max_new_tokens\": 256,\n",
       "  \"max_window_size\": 6144,\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.4,\n",
       "  \"top_k\": 0,\n",
       "  \"top_p\": 0.7\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config.temperature = 0.4\n",
    "model.generation_config.max_new_tokens = 256\n",
    "model.generation_config.top_p = 0.7\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该文本情感是中性的。\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "     \"ST星源2022年12月2日在深交所互动易中披露，截至2022年11月30日公司股东户数为8.99万户，较上期（2022年9月30日）减少447户，减幅为0.49%。ST星源股东户数高于行业平均水平。根据Choice数据，截至2022年11月30日环保行业上市公司平均股东户数为2.95万户。其中，公司股东户数处于0.5万~1.5万区间占比最高，为26.40%。环保行业股东户数分布股东户数与股价2022年6月30日至今，公司股东户数有所下降，区间跌幅为3.17%。2022年6月30日至2022年11月30日区间股价上涨3.85%。股东户数及股价股东户数与股本截至2022年11月30日，公司最新总股本为10.59亿股，其中流通股本为10.58亿股。户均持有流通股数量由上期的1.17万股上升至1.18万股，户均流通市值2.22万元。户均持股金额\",\n",
    "     \"平安银行总经理内部讲话公开，要相信公司前景\",\n",
    "]\n",
    "\n",
    "system_prompt = \"你是一个中文文本分类器，判断以下金融文本的情绪类别，答案为积极、消极或者中性。\"\n",
    "\n",
    "response, history = model.chat(tokenizer, data[0], history = None, system = system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历文件夹中的所有CSV文件\n",
    "folder_path = \"CSI300Data\"\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "def process_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    if 'qwen_lora2_情绪提取' in df.columns:\n",
    "        print(f\"{file_path} already done\")\n",
    "        return\n",
    "    \n",
    "    # 解析日期列并过滤出2019年的数据\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df_2019 = df[df['Date'].dt.year == 2019].copy()\n",
    "\n",
    "    data = df_2019['Title'].fillna('') + \" \" + df_2019['Content'].fillna('')\n",
    "    \n",
    "    responses = []\n",
    "    for text in data:\n",
    "        if text.strip() == \"\":\n",
    "            responses.append(\"\")\n",
    "        else:\n",
    "            response, history = model.chat(tokenizer, text, history=None, system=system_prompt)\n",
    "            responses.append(response)\n",
    "    \n",
    "    df_2019['qwen_lora2_情绪提取'] = responses\n",
    "    print(file_path)\n",
    "    df_2019.to_csv(file_path, index=False, mode=\"w\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # print(df_2019)\n",
    "\n",
    "for csv_file in tqdm(csv_files, desc=\"Overall Progress\"):\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    process_file(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
