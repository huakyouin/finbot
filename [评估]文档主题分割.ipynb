{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据预处理\n",
    "# https://github.com/fjiangAI/CPTS\n",
    "def custom_CPTS(from_dir, to_dir):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    def boundary_to_mass(boundary_list):\n",
    "        bound_idx = [-1] + [i for i, val in enumerate(boundary_list) if val==1]\n",
    "        return [ bound_idx[i+1] - bound_idx[i] for i in range(len(bound_idx) - 1) ]\n",
    "    train_data = pd.read_json(f\"{from_dir}/train.json\")\n",
    "    test_data = pd.read_json(f\"{from_dir}/test.json\")\n",
    "    df = pd.concat([train_data.assign(split='train'),test_data.assign(split='test')], ignore_index=True) # 合并数据集\n",
    "    df['sentences'] = df.apply(lambda row: [i['text'] for i in row['paragraph_list']], axis=1)\n",
    "    df['masses'] = df['label_list'].apply(boundary_to_mass)\n",
    "    # 保存\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df[['id','title','topic_list','sentences', 'masses','split','label_list']].to_json(os.path.join(to_dir,'CPTS.jsonl'), orient='records',force_ascii=False, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configs\n",
    "ebd_model_path = '../../resources/open_models/bge-large-zh-v1.5'\n",
    "seq_model_path = '../../resources/open_models/nlp_bert_document-segmentation_chinese-base'\n",
    "testset_path = \"../resources/data/CPTS.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6中测试指标 https://blog.csdn.net/qq_35082030/article/details/105410478\n",
    "## 开箱即用库 https://github.com/cfournie/segmentation.evaluation\n",
    "## example\n",
    "from segeval.window.pk import pk\n",
    "from segeval.window.windowdiff import window_diff as WD\n",
    "from segeval.similarity.boundary import boundary_similarity as B\n",
    "from segeval.similarity.segmentation import segmentation_similarity as S\n",
    "\n",
    "gold = [2, 3, 6]\n",
    "h_list = [[5, 6], [2, 2, 7], [2, 3, 3, 3], [1, 1, 3, 1, 5]]\n",
    "for n, h in enumerate(h_list):\n",
    "    print(\"第%d次实验\" % int(n + 1))\n",
    "    print(\"1-Pk=%.3f, 1-WD=%.3f, B=%.3f, S=%.3f\" % (pk(h, gold, one_minus=True),\n",
    "            WD(h, gold, one_minus=True, window_size=2), B(h, gold), S(h, gold)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_json(testset_path, lines=True)\n",
    "df = df[df['split']=='test'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from segeval.window.pk import pk\n",
    "from segeval.window.windowdiff import window_diff as WD\n",
    "from segeval.similarity.boundary import boundary_similarity as B\n",
    "from segeval.similarity.segmentation import segmentation_similarity as S\n",
    "\n",
    "import sys; sys.path.append(\"../..\")\n",
    "from utils.spliters import *\n",
    "\n",
    "def init_spliter(method):\n",
    "    if method == \"cos_sim_spliter\":\n",
    "        from FlagEmbedding import FlagModel\n",
    "        model = FlagModel(ebd_model_path, query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\", use_fp16=True)\n",
    "        spliter = BaseSpliter.use_subclass(\"cos_sim_spliter\")(model)\n",
    "\n",
    "    elif method == \"doc_seq_model_spliter\":\n",
    "        from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "        model = AutoModelForTokenClassification.from_pretrained(seq_model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(seq_model_path)\n",
    "        spliter = BaseSpliter.use_subclass(\"doc_seq_model_spliter\")(model, tokenizer)\n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"cos_sim_spliter\"\n",
    "spliter = init_spliter(subject)\n",
    "results = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    text = \"\".join(row['sentences'])\n",
    "    sentence_df = pd.DataFrame(row['sentences'], columns=['sentence'])\n",
    "    sentence_df['start_idx'] = sentence_df['sentence'].apply(lambda x: re.search(re.escape(x), text).start())\n",
    "    sentence_df['end_idx'] = sentence_df['sentence'].apply(lambda x: re.search(re.escape(x), text).end())\n",
    "    chunk_df = spliter.cluster(sentence_df)\n",
    "    pred, gt = chunk_df['sentence_count'].tolist(), row['masses']\n",
    "    pk_score = pk(pred, gt, one_minus=True)\n",
    "    try:\n",
    "        wd_score = WD(pred, gt, one_minus=True, window_size=2)\n",
    "    except Exception as e:\n",
    "        print(f\"WD calculation failed at index {index}: {e}\")\n",
    "        print(f\"pred: {pred}, gt: {gt}\")\n",
    "        wd_score = None\n",
    "    b_score = B(pred, gt)\n",
    "    s_score = S(pred, gt)\n",
    "    \n",
    "    # 将指标保存到结果列表\n",
    "    results.append({\n",
    "        \"index\": index,\n",
    "        \"1-Pk\": pk_score,\n",
    "        \"1-WD\": wd_score,\n",
    "        \"B\": b_score,\n",
    "        \"S\": s_score\n",
    "    })\n",
    "\n",
    "# 转换结果列表为 DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 计算各指标的平均值\n",
    "results_df.loc[\"avg\"] = {\n",
    "    \"index\": \"Average\",\n",
    "    \"1-Pk\": results_df[\"1-Pk\"].mean(),\n",
    "    \"1-WD\": results_df[\"1-WD\"].mean(),\n",
    "    \"B\": results_df[\"B\"].mean(),\n",
    "    \"S\": results_df[\"S\"].mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(f\"results/文档主题分割-{subject}.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
