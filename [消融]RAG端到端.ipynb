{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/icaif-24-finance-rag-challenge/data\n",
    "def translate_FinanceRAG(from_dir, to_dir, llm_config):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    from functools import partial\n",
    "    import sys; sys.path.append(\"..\")\n",
    "\n",
    "    client = OpenAI(base_url=llm_config['base_url'],api_key=llm_config[\"api_key\"])\n",
    "    gen_resp = partial(client.chat.completions.create,model=llm_config['model'],temperature=0.1, top_p=1, max_tokens=1000,)\n",
    "\n",
    "    ## translate_text：翻译指定文本\n",
    "    def translate_text(text):\n",
    "        response = gen_resp(messages=[dict(role=\"system\",content=\"将文字翻译成中文，直接输出翻译结果\"),dict(role=\"user\",content=text)])\n",
    "        return response.choices[0].message.content  \n",
    "\n",
    "    os.makedirs(to_dir,exist_ok=True)\n",
    "    query_df = pd.read_json(os.path.join(from_dir,\"queries.jsonl\"),lines=True)\n",
    "    query_df[\"text_zh\"] = query_df[\"text\"].apply(translate_text)\n",
    "    query_df.to_json(os.path.join(to_dir, \"queries.json\"), orient=\"records\", indent=4, force_ascii=False)\n",
    "    corpus_df = pd.read_json(os.path.join(from_dir,\"corpus.jsonl\"),lines=True)\n",
    "    corpus_df[\"text_zh\"] = corpus_df[\"text\"].apply(translate_text)\n",
    "    corpus_df.to_json(os.path.join(to_dir, \"corpus.json\"), orient=\"records\", indent=4, force_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_FinanceRAG(\n",
    "    \"resources/data/_raw/FinanceBench\",\n",
    "    \"resources/data/ablation_rag/financebench_zh/0_origin\",\n",
    "    dict(model=\"judger\", base_url=\"http://localhost:12235/v1\",api_key=\"empty\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 消融实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 文档主题分割\n",
    "async def split_corpus(work_dirpath, spliter_config):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import sys; sys.path.append(\"..\")\n",
    "    from utils.spliters import init_spliter\n",
    "    import json\n",
    "\n",
    "    # 加载原始数据\n",
    "    corpus_df = pd.read_json(os.path.join(work_dirpath, \"corpus.json\"))\n",
    "\n",
    "    # 初始化分词器\n",
    "    spliter = init_spliter(**spliter_config)\n",
    "\n",
    "    chunk_list = []\n",
    "\n",
    "    for idx, row in corpus_df.iterrows():\n",
    "        sentence_df = spliter.split_text_to_sentences(row['text_zh'])\n",
    "        sentence_df = spliter.add_buffered_sentences(sentence_df)\n",
    "        chunk_df = spliter.cluster(sentence_df)\n",
    "\n",
    "        for _, crow in chunk_df.iterrows():\n",
    "            chunk_text = crow['chunk']\n",
    "            chunk_list.append({\n",
    "                \"doc_id\": row[\"_id\"],\n",
    "                \"doc_text\": row['text_zh'],\n",
    "                \"chunk_text\": chunk_text\n",
    "            })\n",
    "\n",
    "    # 保存 chunk.json\n",
    "    with open(os.path.join(work_dirpath, \"chunk.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "    print(f\"切分完成，共生成 {len(chunk_list)} 个 chunk。\")\n",
    "\n",
    "## 片段摘要生成\n",
    "async def summarize_chunks(work_dirpath, llm_config=None):\n",
    "    import json\n",
    "    import os\n",
    "    import asyncio\n",
    "    from openai import AsyncOpenAI\n",
    "    from minirag.utils import compute_mdhash_id\n",
    "\n",
    "    # 初始化 OpenAI 客户端\n",
    "    client = AsyncOpenAI(base_url=llm_config['base_url'], api_key=llm_config[\"api_key\"]) if llm_config else None\n",
    "    semaphore = asyncio.Semaphore(4)\n",
    "\n",
    "    async def summarize_text(text):\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                response = await client.chat.completions.create(\n",
    "                    model=llm_config['model'],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"为以下内容生成摘要，直接输出结果\"},\n",
    "                        {\"role\": \"user\", \"content\": text}\n",
    "                    ],\n",
    "                    temperature=0.1,\n",
    "                    top_p=1,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"摘要失败: {e}\")\n",
    "                return text[:50]\n",
    "\n",
    "    # 读取 chunk.json\n",
    "    with open(os.path.join(work_dirpath, \"chunk.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        chunk_data = json.load(f)\n",
    "\n",
    "    tasks = []\n",
    "    metadata = []\n",
    "\n",
    "    for item in chunk_data:\n",
    "        chunk_text = item[\"chunk_text\"]\n",
    "        if len(chunk_text) < 50 or client is None:\n",
    "            future = asyncio.Future()\n",
    "            future.set_result(chunk_text)\n",
    "            tasks.append(future)\n",
    "        else:\n",
    "            tasks.append(summarize_text(chunk_text))\n",
    "        metadata.append(item)\n",
    "\n",
    "    summaries = await asyncio.gather(*tasks)\n",
    "\n",
    "    chunk_sum_list = []\n",
    "    for meta, summary in zip(metadata, summaries):\n",
    "        chunk_sum_list.append({\n",
    "            \"doc_id\": meta[\"doc_id\"],\n",
    "            \"chunk_id\": compute_mdhash_id(summary.strip(), prefix=\"chunk-\"),\n",
    "            \"doc_text\": meta[\"doc_text\"],\n",
    "            \"chunk_text\": meta[\"chunk_text\"],\n",
    "            \"chunk_sum_text\": summary,\n",
    "        })\n",
    "\n",
    "    with open(os.path.join(work_dirpath, \"chunk_sum.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_sum_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"摘要完成，共处理 {len(chunk_sum_list)} 个 chunk。\")\n",
    "\n",
    "## 构建图索引\n",
    "async def build_rag_index(work_dirpath, embed_model_path, rag_llm_config):\n",
    "    import os\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from functools import partial\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from minirag import MiniRAG\n",
    "    from minirag.utils import EmbeddingFunc\n",
    "    from minirag.llm import openai_complete_if_cache, hf_embedding\n",
    "    from minirag.prompt import PROMPTS\n",
    "\n",
    "    import sys; sys.path.append(\"..\")\n",
    "    from utils.rag import prompts as custom_prompts\n",
    "\n",
    "    ## 更新 Prompt 词库\n",
    "    PROMPTS.update(custom_prompts)\n",
    "\n",
    "    ## 日志设置\n",
    "    logging.basicConfig(format=\"%(levelname)s: %(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "    ## 准备路径和目录\n",
    "    rag_data_dir = os.path.join(work_dirpath, \"rag_data\")\n",
    "    os.makedirs(rag_data_dir,exist_ok=True)\n",
    "\n",
    "    ## 加载嵌入模型\n",
    "    embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_path, model_max_length=512)\n",
    "    embed_model = AutoModel.from_pretrained(embed_model_path)\n",
    "\n",
    "    ## 初始化 MiniRAG\n",
    "    rag = MiniRAG(\n",
    "        working_dir=rag_data_dir,\n",
    "        llm_model_func=lambda prompt, **kwargs: openai_complete_if_cache(\n",
    "            prompt=prompt, **rag_llm_config, **kwargs\n",
    "        ),\n",
    "        llm_model_name=rag_llm_config[\"model\"],\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=embed_model.config.hidden_size,\n",
    "            max_token_size=embed_model.config.max_position_embeddings,\n",
    "            func=partial(hf_embedding, embed_model=embed_model, tokenizer=embed_tokenizer),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## 加载 chunk summary 文本\n",
    "    chunk_sum_path = os.path.join(work_dirpath, \"chunk_sum.json\")\n",
    "    chunk_sum_df = pd.read_json(chunk_sum_path)\n",
    "    chunk_texts = chunk_sum_df[\"chunk_sum_text\"].tolist()\n",
    "\n",
    "    ## 多条异步插入\n",
    "    batch_size=4\n",
    "    for i in range(0, len(chunk_texts), batch_size):\n",
    "        batch = chunk_texts[i:i+batch_size]\n",
    "        await rag.ainsert(batch)\n",
    "\n",
    "    print(f\"✅ 异步 RAG 索引构建完成，共插入 {len(chunk_texts)} 个 chunk。\")\n",
    "    return rag\n",
    "\n",
    "async def do_rag(work_dirpath, top_k, embed_model_path, rag_llm_config):\n",
    "    import os\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import time\n",
    "    from functools import partial\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from minirag import MiniRAG\n",
    "    from minirag.utils import EmbeddingFunc\n",
    "    from minirag.llm import openai_complete_if_cache, hf_embedding\n",
    "    from minirag.prompt import PROMPTS\n",
    "\n",
    "    import sys; sys.path.append(\"..\")\n",
    "    from utils.rag import prompts as custom_prompts\n",
    "    from utils.rag import get_rag_answer\n",
    "\n",
    "    ## 更新 Prompt 词库\n",
    "    PROMPTS.update(custom_prompts)\n",
    "\n",
    "    ## 日志设置\n",
    "    logging.basicConfig(format=\"%(levelname)s: %(message)s\", level=logging.WARNING)\n",
    "\n",
    "    ## 准备路径和目录\n",
    "    rag_data_dir = os.path.join(work_dirpath, \"rag_data\")\n",
    "    os.makedirs(rag_data_dir,exist_ok=True)\n",
    "\n",
    "    ## 加载嵌入模型\n",
    "    embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_path, model_max_length=512)\n",
    "    embed_model = AutoModel.from_pretrained(embed_model_path)\n",
    "\n",
    "    ## 初始化 MiniRAG\n",
    "    rag = MiniRAG(\n",
    "        working_dir=rag_data_dir,\n",
    "        llm_model_func=lambda prompt, **kwargs: openai_complete_if_cache(\n",
    "            prompt=prompt, **rag_llm_config, **kwargs\n",
    "        ),\n",
    "        llm_model_name=rag_llm_config[\"model\"],\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=embed_model.config.hidden_size,\n",
    "            max_token_size=embed_model.config.max_position_embeddings,\n",
    "            func=partial(hf_embedding, embed_model=embed_model, tokenizer=embed_tokenizer),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## 构建 chunk mapper\n",
    "    chunk_sum_path = os.path.join(work_dirpath, \"chunk_sum.json\")\n",
    "    chunk_sum_df = pd.read_json(chunk_sum_path)\n",
    "    chunk_mapper = dict(zip(chunk_sum_df['chunk_id'], chunk_sum_df['chunk_sum_text']))\n",
    "\n",
    "    ## 读取query列表\n",
    "    query_path = os.path.join(work_dirpath, \"queries.json\")\n",
    "    query_df = pd.read_json(query_path)\n",
    "\n",
    "    answer_list = []\n",
    "    start_time = time.time()\n",
    "    for idx, row in query_df.iterrows():\n",
    "        retrivals, answer = await get_rag_answer(rag, row['text_zh'],chunk_mapper, top_k)\n",
    "        answer_list.append(dict(input=row['text_zh'],retrievals=retrivals,output=answer))\n",
    "    answer_df = pd.DataFrame(answer_list)\n",
    "    answer_df.to_json(os.path.join(work_dirpath,\"answer.json\"),orient=\"records\",index=False, indent=4, force_ascii=False)\n",
    "    elapsed = time.time() - start_time\n",
    "    avg_time = elapsed / len(query_df) if len(query_df) > 0 else 0\n",
    "    answer_stats = dict(query_num=len(query_df),tot_time=elapsed,avg_time=avg_time)\n",
    "    pd.DataFrame([answer_stats]).to_csv(os.path.join(work_dirpath,\"answer_stats.csv\"), index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ RAG执行完成，共回复 {len(query_df)} 个 query，耗时 {elapsed:.2f} 秒，平均响应时长为 {avg_time:.2f} 秒\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: ('seq', 'q25_b3_sft_sum', 'q25_b3_base_index', 'ours', 'q25_b3_base_gen')\n",
      "stage1\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq，跳过\n",
      "stage2\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum，跳过\n",
      "stage3\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index，跳过\n",
      "stage4\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@ours@q25_b3_base_gen，跳过\n",
      "\n",
      "Running: ('seq', 'q25_b3_sft_sum', 'q25_b3_base_index', 'ours', 'q25_b14_gen')\n",
      "stage1\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq，跳过\n",
      "stage2\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum，跳过\n",
      "stage3\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index，跳过\n",
      "stage4\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@ours@q25_b14_gen，跳过\n",
      "\n",
      "Running: ('seq', 'q25_b3_sft_sum', 'q25_b3_base_index', 'naive', 'q25_b3_base_gen')\n",
      "stage1\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq，跳过\n",
      "stage2\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum，跳过\n",
      "stage3\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index，跳过\n",
      "stage4\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@naive@q25_b3_base_gen，跳过\n",
      "\n",
      "Running: ('seq', 'q25_b3_sft_sum', 'q25_b3_base_index', 'lightrag', 'q25_b3_base_gen')\n",
      "stage1\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq，跳过\n",
      "stage2\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum，跳过\n",
      "stage3\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index，跳过\n",
      "stage4\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@lightrag@q25_b3_base_gen，跳过\n",
      "\n",
      "Running: ('seq', 'q25_b3_base_sum', 'q25_b3_base_index', 'ours', 'q25_b3_base_gen')\n",
      "stage1\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq，跳过\n",
      "stage2\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum，跳过\n",
      "stage3\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index，跳过\n",
      "stage4\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index@ours@q25_b3_base_gen，跳过\n",
      "\n",
      "Running: ('seq', 'q25_b14_sum', 'q25_b3_base_index', 'ours', 'q25_b3_base_gen')\n",
      "stage1\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq，跳过\n",
      "stage2\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b14_sum，跳过\n",
      "stage3\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b14_sum@q25_b3_base_index，跳过\n",
      "stage4\n",
      "已存在resources/data/ablation_rag/financebench_zh/seq@q25_b14_sum@q25_b3_base_index@ours@q25_b3_base_gen，跳过\n",
      "\n",
      "Running: ('sim', 'q25_b3_sft_sum', 'q25_b3_base_index', 'ours', 'q25_b3_base_gen')\n",
      "stage1\n",
      "已存在resources/data/ablation_rag/financebench_zh/sim，跳过\n",
      "stage2\n",
      "已存在resources/data/ablation_rag/financebench_zh/sim@q25_b3_sft_sum，跳过\n",
      "stage3\n",
      "已存在resources/data/ablation_rag/financebench_zh/sim@q25_b3_sft_sum@q25_b3_base_index，跳过\n",
      "stage4\n",
      "已存在resources/data/ablation_rag/financebench_zh/sim@q25_b3_sft_sum@q25_b3_base_index@ours@q25_b3_base_gen，跳过\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "stage_funcs = [split_corpus,summarize_chunks,build_rag_index,do_rag]\n",
    "embed_model_path = \"resources/open_models/bge-large-zh-v1.5\"\n",
    "\n",
    "# 各阶段方法配置项\n",
    "split_methods = {\n",
    "    \"seq\": (dict(method=\"doc_seq_model_spliter\", model_path=\"resources/open_models/nlp_bert_document-segmentation_chinese-base\")),\n",
    "    \"sim\": (dict(method=\"cos_sim_spliter\", model_path=\"resources/open_models/bge-large-zh-v1.5\"))\n",
    "}\n",
    "\n",
    "sum_methods = {\n",
    "    \"q25_b3_sft_sum\": (dict(model=\"lora\", base_url=\"http://localhost:12239/v1\", api_key=\"empty\", max_tokens=1000)),\n",
    "    \"q25_b3_base_sum\": (dict(model=\"base\", base_url=\"http://localhost:12239/v1\", api_key=\"empty\", max_tokens=1000)),\n",
    "    \"q25_b14_sum\": (dict(model=\"judger\", base_url=\"http://localhost:12235/v1\", api_key=\"empty\", max_tokens=1000)),\n",
    "}\n",
    "\n",
    "index_methods = {\n",
    "    \"q25_b3_base_index\": (\n",
    "        embed_model_path,\n",
    "        dict(model=\"base\", base_url=\"http://localhost:12239/v1\", api_key=\"empty\", max_tokens=1000)\n",
    "    ),\n",
    "}\n",
    "\n",
    "r_methods = {\n",
    "    \"ours\": ({\"entity\": 1, \"chunk\": 5, \"final\": 5},),\n",
    "    \"naive\": ({\"entity\": 0, \"chunk\": 5, \"final\": 5},),\n",
    "    \"lightrag\" : ({\"entity\": 5, \"chunk\": 5, \"final\": 5},)\n",
    "}\n",
    "\n",
    "g_methods = {\n",
    "    \"q25_b3_base_gen\": (\n",
    "        embed_model_path,\n",
    "        dict(model=\"base\", base_url=\"http://localhost:12239/v1\", api_key=\"empty\", max_tokens=1000)\n",
    "    ),\n",
    "    \"q25_b14_gen\":  (\n",
    "        embed_model_path,\n",
    "        dict(model=\"judger\", base_url=\"http://localhost:12235/v1\", api_key=\"empty\", max_tokens=1000)\n",
    "    ),\n",
    "}\n",
    "\n",
    "rag_methods = {\n",
    "    f\"{r_tag}@{g_tag}\": r_conf + g_conf\n",
    "    for r_tag, r_conf in r_methods.items() \n",
    "    for g_tag, g_conf in g_methods.items()\n",
    "}\n",
    "\n",
    "core_group = (\"seq\", \"q25_b3_sft_sum\",\"q25_b3_base_index\", \"ours\",\"q25_b3_base_gen\")\n",
    "\n",
    "for method_ls in itertools.product(\n",
    "        split_methods.items(), sum_methods.items(), index_methods.items(), rag_methods.items()\n",
    "    ):\n",
    "    current_group_name = \"@\".join([m[0] for m in method_ls])\n",
    "    current_group = tuple(current_group_name.split(\"@\"))\n",
    "    core_diff = sum(a != b for a, b in zip(current_group, core_group))\n",
    "    if core_diff > 1: continue  \n",
    "    \n",
    "    print(f\"\\nRunning: {current_group}\")\n",
    "    for i in range(len(method_ls)):\n",
    "        print(f\"stage{i+1}\")\n",
    "        base_dirpath = \"resources/data/ablation_rag/financebench_zh\"\n",
    "        input_dirname = \"@\".join([m[0] for m in method_ls[:i]]) if i > 0 else \"0_source\"\n",
    "        output_dirname = \"@\".join([m[0] for m in method_ls[:i+1]])\n",
    "        input_dirpath = os.path.join(base_dirpath, input_dirname)\n",
    "        output_dirpath = os.path.join(base_dirpath, output_dirname)\n",
    "        if os.path.exists(output_dirpath):\n",
    "            print(f\"已存在{output_dirpath}，跳过\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"复制上阶段数据到{output_dirpath}\")\n",
    "            shutil.copytree(input_dirpath, output_dirpath,dirs_exist_ok=True)\n",
    "        print(method_ls[i])\n",
    "        await stage_funcs[i](output_dirpath,*method_ls[i][1])\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估答案质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def do_eval(work_dirpath, judge_llm_config):\n",
    "    import sys; sys.path.append(\"..\")\n",
    "    from utils.eval import eval_rag_performance\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    answer_file_path  = os.path.join(work_dirpath,\"answer.json\")\n",
    "    eval_input_df  = pd.read_json(answer_file_path)\n",
    "    eval_result_df, eval_stats_sr = await eval_rag_performance(eval_input_df,judge_llm_config)\n",
    "    eval_result_df.to_json(os.path.join(work_dirpath,\"eval_result.json\"),orient=\"records\",index=False, indent=4, force_ascii=False)\n",
    "    eval_stats_sr.to_json(os.path.join(work_dirpath, \"eval_stats.json\"), force_ascii=False, indent=4)\n",
    "    eval_stats_sr.to_frame().T.to_csv(os.path.join(work_dirpath, \"eval_stats.csv\"), header=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index@naive@q25_b3_base_gen\n",
      "Mean scores:\n",
      "relevance_score       3.50\n",
      "faithfulness_score    3.77\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index@ours@q25_b3_base_gen\n",
      "Mean scores:\n",
      "relevance_score       3.28\n",
      "faithfulness_score    3.50\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index@lightrag@q25_b3_base_gen\n",
      "Mean scores:\n",
      "relevance_score       3.33\n",
      "faithfulness_score    3.71\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index@naive@q25_b14_gen\n",
      "Mean scores:\n",
      "relevance_score       3.55\n",
      "faithfulness_score    3.84\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index@ours@q25_b14_gen\n",
      "Mean scores:\n",
      "relevance_score       3.54\n",
      "faithfulness_score    3.68\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_base_sum@q25_b3_base_index@lightrag@q25_b14_gen\n",
      "Mean scores:\n",
      "relevance_score       3.46\n",
      "faithfulness_score    3.56\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@naive@q25_b3_base_gen\n",
      "Mean scores:\n",
      "relevance_score       2.80\n",
      "faithfulness_score    3.64\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@ours@q25_b3_base_gen\n",
      "Mean scores:\n",
      "relevance_score       2.68\n",
      "faithfulness_score    3.59\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@lightrag@q25_b3_base_gen\n",
      "Mean scores:\n",
      "relevance_score       2.67\n",
      "faithfulness_score    3.65\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@naive@q25_b14_gen\n",
      "Mean scores:\n",
      "relevance_score       3.0\n",
      "faithfulness_score    3.5\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b3_sft_sum@q25_b3_base_index@ours@q25_b14_gen\n",
      "Mean scores:\n",
      "relevance_score       2.87\n",
      "faithfulness_score    3.41\n",
      "dtype: float64\n",
      "当前评估对象： resources/data/ablation_rag/financebench_zh/seq@q25_b14_sum@q25_b3_base_index@ours@q25_b3_base_gen\n",
      "Mean scores:\n",
      "relevance_score       3.41\n",
      "faithfulness_score    3.76\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "base_dirpath = \"resources/data/ablation_rag/financebench_zh\"\n",
    "eval_dirs = []\n",
    "for d in os.listdir(base_dirpath):\n",
    "    full_path = os.path.join(base_dirpath, d)\n",
    "    if os.path.isdir(full_path) and d.count('@') == 4:\n",
    "        eval_dirs.append(full_path)\n",
    "\n",
    "for eval_dir in eval_dirs:\n",
    "    if os.path.exists(os.path.join(eval_dir,\"eval_result.json\")): continue\n",
    "    print(\"当前评估对象：\", eval_dir)\n",
    "    await do_eval(eval_dir, dict(model=\"judger\", base_url=\"http://localhost:12235/v1\", api_key=\"empty\", max_tokens=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
