{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer, AdamW\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, bert_path_or_name, num_labels=2):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_path_or_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "        # 冻结 BERT 的所有参数\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 获取 BERT 的输出\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # 使用 [CLS] token 的输出\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "        # 通过分类头\n",
    "        logits = self.classifier(cls_output)  # (batch_size, num_labels)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CLSDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        Args:\n",
    "            texts (list): 包含文本的列表\n",
    "            labels (list): 文本对应的标签列表\n",
    "            tokenizer (PreTrainedTokenizer): 分词器实例\n",
    "            max_length (int): 文本最大长度，超过此长度将会截断\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取数据样本，并将文本转为 BERT 可用的输入格式\n",
    "        Args:\n",
    "            idx (int): 索引值\n",
    "        Returns:\n",
    "            dict: 包含 `input_ids`, `attention_mask`, `labels` 的字典\n",
    "        \"\"\"\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 对文本进行编码\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 获取编码结果并移除不必要的维度\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer, device):\n",
    "    \"\"\"训练模型并返回平均训练损失\"\"\"\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "    total_loss = 0.0  # 初始化总训练损失\n",
    "    total_correct = 0  # 初始化正确预测的数量 \n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 前向传播和计算损失\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累加当前 batch 的总损失\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "         # 计算预测正确的样本数量\n",
    "        _, predicted = torch.max(logits, dim=1)  # 取 logits 中的最大值作为预测类别\n",
    "        total_correct += (predicted == labels).sum().item()  # 统计正确预测数量\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def eval(model, data_loader, criterion, device):\n",
    "    \"\"\"评估模型并返回平均验证损失\"\"\"\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0  # 初始化正确预测的数量\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 前向传播计算验证损失\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            val_loss = criterion(logits, labels)\n",
    "\n",
    "            # 累加当前 batch 的总验证损失\n",
    "            total_loss += val_loss.item() * input_ids.size(0)\n",
    "\n",
    "            # 计算预测正确的样本数量\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/FinBert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertBinaryClassifier(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理过数据, 直接加载...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "data_name = \"Dataset-of-financial-news-sentiment-classification\"\n",
    "if os.path.exists(f\"../data/cleaned/{data_name}.jsonl\"):\n",
    "    print(\"已处理过数据, 直接加载...\")\n",
    "    df = pd.read_json(f\"../data/cleaned/{data_name}.jsonl\", lines=True)\n",
    "else:\n",
    "    print(\"加载原始数据集...\")\n",
    "    train_data = pd.read_csv(f\"../data/raw/{data_name}/train_data.csv\")\n",
    "    test_data = pd.read_csv(f\"../data/raw/{data_name}/test_data.csv\")\n",
    "    get_text = lambda x: x['正文']\n",
    "    get_label = lambda x: x['正负面']\n",
    "    # 合并数据集\n",
    "    df = pd.concat([\n",
    "        train_data.assign(split='train', text=get_text, label=get_label),\n",
    "        test_data.assign(split='test', text=get_text, label=get_label)\n",
    "    ], ignore_index=True)[['split', 'text', 'label']]\n",
    "    # 清洗\n",
    "    df = df[df['text'].notna() & (df['text'] != '')]\n",
    "    # 保存清洗后的数据\n",
    "    df.to_json(f'../data/cleaned/{data_name}.jsonl', index=False, orient='records',force_ascii=False, lines=True)\n",
    "\n",
    "train_df = df.query(\"split == 'train'\")\n",
    "\n",
    "train_ds = CLSDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    texts = train_df['text'].to_list(),\n",
    "    labels = train_df['label'].to_list(),\n",
    "    max_length = 512,\n",
    ")\n",
    "\n",
    "train_size = int(0.7 * len(train_ds))  # 70% 的训练数据\n",
    "val_size = len(train_ds) - train_size   # 剩余的 30% 作为验证数据\n",
    "train_subset, val_subset = random_split(train_ds, [train_size, val_size]) \n",
    "\n",
    "batch_size = 16  # 设置批量大小\n",
    "train_dl = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_subset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "盛运环保2月13日晚间发布公告称，截至目前，共有37.48亿元到期债务未清偿。\n",
      "[CLS] 盛 运 环 保 2 月 13 日 晚 间 发 布 公 告 称 ， 截 至 目 前 ， 共 有 37. 48 亿 元 到 期 债 务 未 清 偿 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,'text'])\n",
    "print(tokenizer.decode(train_ds[0]['input_ids'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/JXH/01_apps/miniforge3/envs/finbot/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=2e-5)  # 只更新分类头的参数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 开启训练\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "plt.figure(dpi=500)  # 设置图形的 DPI (每英寸点数)\n",
    "\n",
    "# 训练和评估模型\n",
    "train_losses = []  # 用于存储每个 epoch 的训练平均 loss\n",
    "val_losses = []    # 用于存储每个 epoch 的验证平均 loss\n",
    "train_accuracies = []  # 用于存储每个 epoch 的训练精确度\n",
    "val_accuracies = []    # 用于存储每个 epoch 的验证精确度\n",
    "\n",
    "# plt.ion()  # 开启交互模式\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_train_loss, train_accuracy = train(model, train_dl, criterion, optimizer, device)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    avg_val_loss, val_accuracy = eval(model, val_dl, criterion, device)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,3))\n",
    "    fig.subplots_adjust(hspace=0.3)  # 调整子图间隔\n",
    "    # 更新损失图\n",
    "    ax1.plot(train_losses, label='Training Loss', color='blue')\n",
    "    ax1.plot(val_losses, label='Validation Loss', color='orange')\n",
    "    ax1.set_xlim(0, epoch + 1)  # 动态调整 x 轴\n",
    "    ax1.set_ylim(0, max(max(train_losses), max(val_losses, default=0), 1e-10))  # 动态调整 y 轴\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "\n",
    "    # 更新准确度图\n",
    "    ax2.plot(train_accuracies, label='Training Accuracy', color='green')\n",
    "    ax2.plot(val_accuracies, label='Validation Accuracy', color='red')\n",
    "    ax2.set_xlim(0, epoch + 1)  # 动态调整 x 轴\n",
    "    ax2.set_ylim(0, 1)  # 精确度范围在 [0, 1]\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    \n",
    "    plt.pause(1e-9)  # 暂停以更新图形\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Average Training Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Training Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Average Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
