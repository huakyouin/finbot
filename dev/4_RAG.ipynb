{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk2/xinghua.jia/.miniforge3/envs/finbot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Config\n",
    "from minirag.prompt import PROMPTS\n",
    "import sys; sys.path.append(\"..\")\n",
    "from utils.prompts import rag_prompts\n",
    "\n",
    "PROMPTS.update(rag_prompts) # 提示词模板\n",
    "LLM_MODEL = \"test\"  # 模型名称\n",
    "DATA_PATH = \"../resources/data/LiHua-World/data/\"  # 数据路径\n",
    "QUERY_PATH = \"../resources/data/LiHua-World/qa/query_set.csv\"  # 查询路径\n",
    "WORKING_DIR = \"../resources/data/rag_outputs\" # 工作目录\n",
    "OUTPUT_PATH = \"./logs/Default_output.csv\"  # 输出路径\n",
    "EMBED_MODEL_PATH = \"../resources/open_models/bge-large-zh-v1.5\" # 嵌入模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Logger initialized for working directory: ../resources/data/rag_outputs\n",
      "INFO:minirag:Load KV llm_response_cache with 14 data\n",
      "INFO:minirag:Load KV full_docs with 3 data\n",
      "INFO:minirag:Load KV text_chunks with 3 data\n",
      "INFO:minirag:Loaded graph from ../resources/data/rag_outputs/graph_chunk_entity_relation.graphml with 6 nodes, 3 edges\n",
      "INFO:nano-vectordb:Load (5, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '../resources/data/rag_outputs/vdb_entities.json'} 5 data\n",
      "INFO:nano-vectordb:Load (5, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '../resources/data/rag_outputs/vdb_entities_name.json'} 5 data\n",
      "INFO:nano-vectordb:Load (2, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '../resources/data/rag_outputs/vdb_relationships.json'} 2 data\n",
      "INFO:nano-vectordb:Load (5, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': '../resources/data/rag_outputs/vdb_chunks.json'} 5 data\n"
     ]
    }
   ],
   "source": [
    "## 初始化\n",
    "import os\n",
    "import csv\n",
    "from tqdm import trange\n",
    "from minirag import MiniRAG, QueryParam\n",
    "from minirag.llm import openai_complete_if_cache, hf_embedding\n",
    "from minirag.utils import EmbeddingFunc\n",
    "import logging\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nest_asyncio; nest_asyncio.apply() # 在notebook中使用async所需\n",
    "\n",
    "# 设置日志级别\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_PATH, model_max_length=512) \n",
    "embed_model = AutoModel.from_pretrained(EMBED_MODEL_PATH)\n",
    "\n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n",
    ") -> str:\n",
    "    return await openai_complete_if_cache(\n",
    "        model=\"base\",\n",
    "        api_key=\"empty\",\n",
    "        base_url=\"http://localhost:12239/v1\",\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "async def embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    return await hf_embedding(\n",
    "        texts,\n",
    "        embed_model=embed_model,\n",
    "        tokenizer=embed_tokenizer,\n",
    "    )\n",
    "\n",
    "os.makedirs(WORKING_DIR,exist_ok=True)\n",
    "\n",
    "rag = MiniRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=llm_model_func,\n",
    "    llm_model_max_token_size=1000,\n",
    "    llm_model_name=LLM_MODEL,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=embed_model.config.hidden_size,\n",
    "        max_token_size=embed_model.config.max_position_embeddings,\n",
    "        func=embedding_func\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:minirag:All docs are already in the storage\n",
      "WARNING:minirag:All docs are already in the storage\n",
      "WARNING:minirag:All docs are already in the storage\n",
      "INFO:minirag:[New Docs] inserting 1 docs\n",
      "INFO:minirag:[New Chunks] inserting 1 chunks\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n",
      "INFO:minirag:[Entity Extraction]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/442\n",
      "1/442\n",
      "2/442\n",
      "3/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:12237/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:12237/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 2 entities(duplicated), 1 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 2 vectors to entities\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (0) is identical to the `bos_token_id` (0), `eos_token_id` (2), or the `sep_token_id` (None), and your input is not padded.\n",
      "INFO:minirag:Inserting 2 vectors to entities_name\n",
      "INFO:minirag:Inserting 1 vectors to relationships\n",
      "INFO:minirag:Writing graph with 6 nodes, 3 edges\n",
      "INFO:minirag:[New Docs] inserting 1 docs\n",
      "INFO:minirag:[New Chunks] inserting 1 chunks\n",
      "INFO:minirag:Inserting 1 vectors to chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4/442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:[Entity Extraction]...\n",
      "INFO:httpx:HTTP Request: POST http://localhost:12237/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:12237/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ Processed 1 chunks, 2 entities(duplicated), 3 relations(duplicated)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:minirag:Inserting 2 vectors to entities\n",
      "INFO:minirag:Inserting 2 vectors to entities_name\n",
      "INFO:minirag:Inserting 3 vectors to relationships\n",
      "INFO:minirag:Writing graph with 7 nodes, 5 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 构建索引\n",
    "def find_txt_files(root_path):\n",
    "    txt_files = []\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                txt_files.append(os.path.join(root, file))\n",
    "    return txt_files\n",
    "\n",
    "WEEK_LIST = find_txt_files(DATA_PATH)\n",
    "for WEEK in WEEK_LIST[:5]:\n",
    "    id = WEEK_LIST.index(WEEK)\n",
    "    print(f\"{id}/{len(WEEK_LIST)}\")\n",
    "    with open(WEEK) as f:\n",
    "        content = f.read()\n",
    "        rag.insert(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:12237/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiHua's prediction about events in \"The Rings of Power\" is not provided in the given information. The provided data mainly covers a dialogue on the issue of a broken water tap in an apartment, the confirmation that a plumber will arrive, and the interaction with Adam Smith who provides assistance and updates. There is no mention or indication regarding any predictions or storyline developments in \"The Rings of Power\" from which to derive LiHua's forecast.\n"
     ]
    }
   ],
   "source": [
    "## 查询与回答\n",
    "query = \"What does LiHua predict will happen in \\\"The Rings of Power\\\"?\"\n",
    "answer = rag.query(query, param=QueryParam(mode=\"mini\")).replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "fileId": "2a9aae82-8ee1-4004-a5a7-13cae40dba29",
  "filePath": "/mlx_devbox/users/jiaxinghua/playground/01_workspace/MiniRAG/RAG.ipynb",
  "kernelspec": {
   "display_name": "finbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
