{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"中兴通讯高级副总裁徐锋：5G时代中兴手机希望重回主流队列, 12月13日，在中兴通讯上海研究所，中兴通讯高级副总裁、中兴终端事业部总裁徐锋在接受澎湃新闻记者采访时表示，5G时代，中兴通讯整体策略是“5G先锋”，希望引领技术创新，包括从系统到终端、完整的5G端到端的解决方案。作为中兴通讯终端业务负责人，徐锋希望5G时代中兴通讯终端能够重新回到主流手机品牌队列。“明年我们在全球会推出10款左右的5G手机，15款以上的5G终端(包括手机和其他形态)。就手机而言，会覆盖2000元到3000元的价位段。”徐锋透露。明年手机市场洗牌是大概率事件徐锋称，2020年是5G手机市场全面爆发的第一年，根据不同的咨询机构预测的情况，加上中兴通讯自己的判断，明年全球5G终端大概会在1.6亿台左右，中国至少占据一半，在8000万到1亿台之间。“所以中国肯定是最主要、最早的市场。5G终端毫无疑问中国市场是主战场。”徐锋表示，中国市场渠道是比较成熟的，中兴通讯线上线下的渠道都会拓展。但他强调，相比4G，运营商渠道在5G初期的主导力会加强，因此跟运营商渠道的合作是非常重要的一环。徐锋认为，明年手机市场洗牌是大概率事件，主要原因是5G技术变化非常大，从4G到5G，技术上是非常大的跨度；另外，应用场景的变化非常大，从纯粹“带宽增加”拓展到“广连接”、“高可靠”、“低时延”的场景，应用场景大大扩展。“在巨大的变化下，也意味着很多机会或者可能出现的问题，所以变局的可能性很大。”那么，中兴手机如何才能突围？徐锋称，手机厂商希望在终端上能够做到领先，最重要的是对5G要有非常深刻的理解。因为5G可能会比以往更多地改变人类的生活、改变社会，所以需要对5G有非常深刻的理解。其次，手机厂商需要利用对5G的理解给用户创造更多的价值，只有抓住这两点才可能领先于别人，比别人更成功。“这是我们希望在5G时代能够突围的方式。当然，传统短板也要弥补，相对来说中兴在国内品牌和渠道方面是相对弱势的，所以这是我们要弥补的。但是靠弥补不可能取胜，只有更多地发挥强项才有可能实现突围。”徐锋1998年加入中兴通讯股份有限公司，2014年加入终端事业部，2018年7月开始担任中兴通讯高级副总裁兼终端事业部总裁，负责中兴全球终端业务。明年推10款5G终端覆盖2000-3000元价位段\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/JXH/01_apps/miniforge3/envs/finbot/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/disk1/JXH/01_apps/miniforge3/envs/finbot/lib/python3.8/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/mnt/disk1/JXH/01_apps/miniforge3/envs/finbot/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中兴通讯高级副总裁徐锋：5G时代中兴手机希望重回主流队列, 12月13日，在中兴通讯上海研究...</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>明年手机市场洗牌是大概率事件徐锋称，2020年是5G手机市场全面爆发的第一年，根据不同的咨询...</td>\n",
       "      <td>255</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>徐锋认为，明年手机市场洗牌是大概率事件，主要原因是5G技术变化非常大，从4G到5G，技术上是...</td>\n",
       "      <td>481</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>其次，手机厂商需要利用对5G的理解给用户创造更多的价值，只有抓住这两点才可能领先于别人，比别...</td>\n",
       "      <td>724</td>\n",
       "      <td>944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>明年推10款5G终端覆盖2000-3000元价位段</td>\n",
       "      <td>944</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  start_idx  end_idx\n",
       "0  中兴通讯高级副总裁徐锋：5G时代中兴手机希望重回主流队列, 12月13日，在中兴通讯上海研究...          0      255\n",
       "1  明年手机市场洗牌是大概率事件徐锋称，2020年是5G手机市场全面爆发的第一年，根据不同的咨询...        255      481\n",
       "2  徐锋认为，明年手机市场洗牌是大概率事件，主要原因是5G技术变化非常大，从4G到5G，技术上是...        481      724\n",
       "3  其次，手机厂商需要利用对5G的理解给用户创造更多的价值，只有抓住这两点才可能领先于别人，比别...        724      944\n",
       "4                          明年推10款5G终端覆盖2000-3000元价位段        944      969"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.split_text import *\n",
    "model_name = '../models/nlp_bert_document-segmentation_chinese-base'\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "spliter = BaseSpliter.use_subclass(\"doc_seg_model_spliter\")(model, tokenizer)\n",
    "spliter.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from FlagEmbedding import FlagModel\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.split_text import *\n",
    "model = FlagModel('../models/bge-large-zh-v1.5', query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\", use_fp16=True)\n",
    "spliter = BaseSpliter.use_subclass(\"cos_sim_spliter\")(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"中兴通讯高级副总裁徐锋：5G时代中兴手机希望重回主流队列, 12月13日，在中兴通讯上海研究所，中兴通讯高级副总裁、中兴终端事业部总裁徐锋在接受澎湃新闻记者采访时表示，5G时代，中兴通讯整体策略是“5G先锋”，希望引领技术创新，包括从系统到终端、完整的5G端到端的解决方案。作为中兴通讯终端业务负责人，徐锋希望5G时代中兴通讯终端能够重新回到主流手机品牌队列。“明年我们在全球会推出10款左右的5G手机，15款以上的5G终端(包括手机和其他形态)。就手机而言，会覆盖2000元到3000元的价位段。”徐锋透露。明年手机市场洗牌是大概率事件徐锋称，2020年是5G手机市场全面爆发的第一年，根据不同的咨询机构预测的情况，加上中兴通讯自己的判断，明年全球5G终端大概会在1.6亿台左右，中国至少占据一半，在8000万到1亿台之间。“所以中国肯定是最主要、最早的市场。5G终端毫无疑问中国市场是主战场。”徐锋表示，中国市场渠道是比较成熟的，中兴通讯线上线下的渠道都会拓展。但他强调，相比4G，运营商渠道在5G初期的主导力会加强，因此跟运营商渠道的合作是非常重要的一环。徐锋认为，明年手机市场洗牌是大概率事件，主要原因是5G技术变化非常大，从4G到5G，技术上是非常大的跨度；另外，应用场景的变化非常大，从纯粹“带宽增加”拓展到“广连接”、“高可靠”、“低时延”的场景，应用场景大大扩展。“在巨大的变化下，也意味着很多机会或者可能出现的问题，所以变局的可能性很大。”那么，中兴手机如何才能突围？徐锋称，手机厂商希望在终端上能够做到领先，最重要的是对5G要有非常深刻的理解。因为5G可能会比以往更多地改变人类的生活、改变社会，所以需要对5G有非常深刻的理解。其次，手机厂商需要利用对5G的理解给用户创造更多的价值，只有抓住这两点才可能领先于别人，比别人更成功。“这是我们希望在5G时代能够突围的方式。当然，传统短板也要弥补，相对来说中兴在国内品牌和渠道方面是相对弱势的，所以这是我们要弥补的。但是靠弥补不可能取胜，只有更多地发挥强项才有可能实现突围。”徐锋1998年加入中兴通讯股份有限公司，2014年加入终端事业部，2018年7月开始担任中兴通讯高级副总裁兼终端事业部总裁，负责中兴全球终端业务。明年推10款5G终端覆盖2000-3000元价位段\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用于在RAG中对文本进行分片的函数，不包含embedding\n",
    "'''\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_text_to_sentences(text,split_chars=\".?!。\\n！：\"):\n",
    "    ''' 按分割符切割文本成句子'''\n",
    "    pattern = re.compile(f'(?<=[{split_chars}])\\s+')\n",
    "    single_sentences_list = pattern.split(text)\n",
    "\n",
    "    # 去除列表中的空字符串项，并记录每个句子在原文中的起始和结束位置\n",
    "    start_idx = 0\n",
    "\n",
    "    sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "\n",
    "    for item in sentences:\n",
    "        item[\"length\"] = len(item['sentence'])\n",
    "        item['start_idx'] = start_idx\n",
    "        start_idx += item[\"length\"]\n",
    "        # 跳过分隔符长度\n",
    "        while start_idx < len(text) and text[start_idx] in split_chars:\n",
    "            start_idx += 1\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    '''\n",
    "    把[i-buffer_size,i]范围内的句子对象合并，返回一个纯句子列表\n",
    "    Args:\n",
    "        sentences:      句子对象构成的列表\n",
    "        buffer_size:    缓冲区长度\n",
    "    Returns:\n",
    "        combined_sentences(list)\n",
    "    '''\n",
    "    combined_sentences = [\n",
    "        ' '.join(sentences[j]['sentence'] for j in range(max(i - buffer_size, 0), min(i + 1, len(sentences))))\n",
    "        for i in range(len(sentences))\n",
    "    ]   \n",
    "\n",
    "    return combined_sentences\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"计算两个向量的余弦相似度\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def calculate_cosine_distances(vec1_list, vec2_list):\n",
    "    \"\"\"\n",
    "    计算两组向量之间的余弦距离。\n",
    "    \n",
    "    Args:\n",
    "        vec1_list (list of arrays): 第一组向量的列表。\n",
    "        vec2_list (list of arrays): 第二组向量的列表，与第一组向量对应。\n",
    "\n",
    "    Returns:\n",
    "        distances (list): 每对向量之间的余弦距离的列表。\n",
    "    \"\"\"\n",
    "    assert len(vec1_list) == len(vec2_list), \"向量列表长度不一致\"\n",
    "    distances = []\n",
    "    n = len(vec1_list)\n",
    "    for i in range(n):\n",
    "        # 计算余弦相似度\n",
    "        similarity = cosine_similarity(vec1_list[i], vec2_list[i])\n",
    "        # 转换为余弦距离\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "    return distances\n",
    "\n",
    "\n",
    "def build_chunks(text, sentences,distances,breakpoint_percentile_threshold=95,max_len=450):\n",
    "    '''\n",
    "    根据距离把文本分片\n",
    "    Input:\n",
    "        text:       原文\n",
    "        sentences:  句子列表\n",
    "        distances:  跟下句的相似距离\n",
    "        breakpoint_percentile_threshold：   距离分割阈值分位数（0-100）\n",
    "        max_len:    最大片段长度\n",
    "    '''\n",
    "    if len(sentences)<=1: # 全文被分为一句话\n",
    "        return [{\"start_idx\":sentences[0][\"start_idx\"], \"text\": text}]\n",
    "\n",
    "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)\n",
    "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] + [len(sentences)-1] + [\"end\"]\n",
    "\n",
    "    chunks = []\n",
    "    start_sent_idx = 0\n",
    "    pin = 0\n",
    "    last_sent_idx = indices_above_thresh[pin]\n",
    "    while last_sent_idx!=\"end\":\n",
    "        combined_text = text[sentences[start_sent_idx][\"start_idx\"]: sentences[last_sent_idx][\"start_idx\"]+sentences[last_sent_idx][\"length\"]]\n",
    "        # print(len(combined_text))\n",
    "        if len(combined_text) < max_len or last_sent_idx==start_sent_idx:\n",
    "            chunk = {\"text\":combined_text,\"sentence_range\":[start_sent_idx,last_sent_idx]}\n",
    "            chunks.append(chunk)\n",
    "            # 更新循环变量 \n",
    "            start_sent_idx = last_sent_idx + 1\n",
    "            pin += indices_above_thresh[pin]==last_sent_idx  # 如果当前分裂原因不是max_len，就+1\n",
    "            last_sent_idx = indices_above_thresh[pin]\n",
    "        else:\n",
    "            last_sent_idx = start_sent_idx + np.argmax(distances[start_sent_idx:last_sent_idx])  # 不含last_sent_idx\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def find_emb_matched_chunks_ids(query_embeddings, chunk_embeddings, topn=10, min_similarity=0.51,relaxation_limit=2):\n",
    "    \"\"\"\n",
    "    寻找匹配片段\n",
    "\n",
    "    Args:\n",
    "        query_embeddings (list): 查询片段的嵌入列表\n",
    "        chunk_embeddings (list): 上下文片段的嵌入列表\n",
    "        topn (int, optional): 返回每个查询片段的最大匹配数\n",
    "        min_similarity (float, optional): 最小的余弦相似度阈值，用于确定匹配\n",
    "        relaxation_limit (int, optional): 如果没有匹配的片段时的松弛限制数量\n",
    "\n",
    "    Returns:\n",
    "        list: 匹配片段的索引列表，每个查询片段对应一个子列表    (n,*)二维列表，n为查询数\n",
    "    \"\"\"\n",
    "    matched_indices = []\n",
    "\n",
    "    for query_embedding in query_embeddings:\n",
    "        temp = []\n",
    "\n",
    "        # 计算查询片段与所有上下文片段的余弦相似度\n",
    "        similarities = [cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in chunk_embeddings]\n",
    "\n",
    "        # 对相似度数组进行排序并获取排序后的索引\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "        # 选择符合最小相似度阈值且未超过匹配数的索引\n",
    "        for idx in sorted_indices:\n",
    "            if similarities[idx] > min_similarity :\n",
    "                temp.append(idx)\n",
    "        \n",
    "        # 如果召回为0则根据relaxation_limit设定松弛\n",
    "        if len(temp)==0:\n",
    "            temp += sorted_indices[:relaxation_limit]\n",
    "\n",
    "        matched_indices.append(temp[:topn])\n",
    "\n",
    "    return matched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from FlagEmbedding import FlagModel\n",
    "import torch\n",
    "model = FlagModel('../models/bge-large-zh-v1.5', query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例文本\n",
    "text = \"青藏高原之后，紧接着是云贵高原、内蒙古高原以及西北内陆地区太阳辐射量较高；而华东、华南地区辐照度较低，最低者便是文章开头提到的四川盆地。\\n\\n\\n\\n下面是按照民族和地区对人群肤色进行的分析：普遍规律是：\\n\\n生活在辐射高地区的人群肤色深\\n\\n生活在辐射低地区的人群肤色浅\\n\\n\\n\\n如果吸收太多紫外线辐射，会让皮肤皱纹增加，变得粗糙、松垂、缺乏弹性。过强紫外线照射还会导致皮肤不均匀地增厚和变薄，常见的阳光引起的皮肤色素变化是雀斑和晒斑，这两者都是长时间暴露在日光下导致的。\\n\\n\\n\\n像是海南三亚的太阳紫外线辐射是沈阳的两倍，但三亚男性皮肤老化的风险是沈阳男性的约6倍，女性则达到了惊人的11倍。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n当然了地理君温馨提醒：\\n\\n以上说的都是普遍性，假如你生活在四川盆地皮肤却很黑，那只能说明你：没做好防晒！\\n\\n\\n气候湿润地区的高颜值“密码”\\n\\n\\n\\n除了皮肤颜色外，令人羡慕的优质皮肤还需满足两个条件：一是皮肤的含水量要足够多；二是皮肤出油率要保持较低水平。\\n\\n\\n\\n皮肤的含水量跟湿度密切相关，湿度过低，人体皮肤因缺少水分而变得粗糙甚至开裂，人体的免疫系统也会受到伤害，对疾病的抵抗力大大降低甚至丧失；\\n\\n\\n\\n而湿度过高时，并且如果长久呆在潮湿空气中，人体皮肤会出现少量细菌感染或者皮肤发痒等现象。所以长时间呆在湿度过高或者过低的环境里都是不利于皮肤的发育。\\n\\n\\n\\n\\n通常情况下，人体皮肤所需的舒适气温在10~22℃之间，这相当于皮肤所需的舒适湿度在25%~45%之间。\\n\\n\\n\\n1月是冬季的典型代表，皮肤最喜欢的相对湿度45%等值线大体沿25°N分布，也就是南岭以南的华南地区。\\n\\n\\n\\n\\n5月是春季的代表，此时也是全国湿度的舒适范围达到最大的月份，除了东北北部和青藏高原腹地外，全国都在舒适带范围内。\\n\\n\\n\\n7月是夏季的代表，“热”字当头，除了东北北部和青藏高原外，全国其它地区都不在舒适带范围内。\\n\\n\\n\\n\\n以9月、10月为主的秋季，全国舒适带的范围和春季的分布相差无几。\\n\\n\\n\\n那么哪个地区相对而言湿度要更大一些呢？\\n\\n\\n\\n答案是：川渝地区仍然榜上有名。\\n\\n\\n\\n总体上，根据2021年《中国统计年鉴》的数据，西南地区的平均相对湿度更舒适一些，这样的湿度更利于形成或维持水润的皮肤。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n地理君再次温馨提醒：\\n\\n皮肤适宜的舒适湿度同样具有普遍性，假如你生活在川渝地区皮肤却很干或者皱纹很多，那说明你：有可能是吸烟人群。\\n\\n图片\\n\\n\\n\\n\\n\\n\\n除此以外，由于地域辽阔，自然环境特征复杂多样，普遍性中往往有很多的个例。\\n\\n\\n\\n例如，长春的冬季漫长、寒冷干燥，气候条件比很多南方城市还要恶劣，但正是因为冬季严寒，当地人们反而在户外活动的时间变少，防寒保暖的措施更加牢固，因此长春人皮肤的水分状况反而要比江苏、湖南等地的人们的皮肤好。\\n\\n\\n\\n吸烟有害健康大家都知道，但是吸烟对皮肤的伤害也很大，甚至超过了阳光带给皮肤的伤害。\\n\\n\\n\\n吸烟会导致皮肤的新陈代谢功能降低，易于发生皮肤老化。还会降低人体胶原合成的减少，易于皮肤产生皱纹。\\n\\n\\n\\n\\n当今社会，中国的平均吸烟率在25%左右，但是男女吸烟率差距极大，男性吸烟率接近50%，女性吸烟率仅3.1%左右。但是15岁以上人群中吸烟者在2018年便超过了总吸烟人数的四分之一。\\n\\n\\n\\n一篇发表在《柳叶刀》的文章研究称：我国最喜欢吸烟的人群主要分布在西北地区，比如宁夏银川人吸烟率就高达49.8%。\\n\\n\\n\\n而南方一些省市的吸烟率，例如贵州省超过了40%，江浙沪、云南四川等地对烟草的依赖则最低，总体上是北方多于南方。\\n\\n\\n\\n\\n除了吸烟会影响皮肤状况外，皮肤出油率高、经常爆痘也是其重要影响因素。\\n\\n\\n\\n时不时长个痘，难受不？该不该挤呢？\\n\\n\\n\\n实话实说，痘痘的发病率是有明显的地域差别。跟人们普遍认知不同的是，南方的爆痘率要明显高于北方。\\n\\n\\n\\n华南地区更是长痘的重灾区。这是因为湿度过高、气温炎热的广州、三亚等地，人们的皮脂分泌通常也较旺盛，皮肤相对油腻、容易爆痘。\\n\\n\\n\\n那么，全国最不长痘的地方在哪里呢？\\n\\n\\n\\n依旧是川渝所在的西南地区，不愧“天府之国”啊！\\n\\n\\n\\n不止在爆痘率上，出油率南方也是高于北方地区。北方气候比较冷，空气干燥。皮肤毛孔收缩，保持皮肤的含水量，皮脂腺分泌降低，皮肤内的水分也用来保持皮肤湿润，因此皮肤油脂少。\\n\\n\\n\\n而南方空气相较于北方要暖和且空气湿润，因此皮肤新陈代谢快，皮脂腺分泌旺盛，就导致了皮肤容易出油。\\n\\n\\n\\n其实出油并不代表皮肤水分充足，恰恰相反，正是由于人体皮肤干燥缺水，导致肌肤受损，人体自我修复和保护系统才会分泌油脂来保护肌肤！所以即使在南方也要做到皮肤及时补水。\\n\\n\\n\\n一个地区的人的皮肤状况和自然地理环境有一定联系，而且这种联系也是潜移默化的，但是随着人口流动的增加，导致基因多元化，所以二者联系越来越小\"\n",
    "\n",
    "\n",
    "sentences = split_text_to_sentences(text)\n",
    "\n",
    "# 打印结果\n",
    "for item in sentences[:2]:\n",
    "    idx, sentence, start = item[\"index\"],item['sentence'],item[\"start_idx\"]\n",
    "    print(f\"Sentence {idx+1}\")\n",
    "    print(sentence)\n",
    "    print(\"------\")\n",
    "    #print(f\"Start Index: {start}\")\n",
    "    print(text[start:start+len(sentence)],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentences_pre1 = combine_sentences(sentences,buffer_size=1)\n",
    "combined_sentences_pre2 = combine_sentences(sentences,buffer_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "combined_sentence_pre1_embeddings = model.encode(combined_sentences_pre1)\n",
    "combined_sentence_pre2_embeddings = model.encode(combined_sentences_pre2)\n",
    "distances = calculate_cosine_distances(combined_sentence_pre1_embeddings[:-1],combined_sentence_pre2_embeddings[1:])\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_lib = {\"large\":build_chunks(text,sentences,distances,75),\"small\":build_chunks(text,sentences,distances,30)}\n",
    "# 定义一个小到大映射\n",
    "fa = [0] * len(chunks_lib[\"small\"])\n",
    "for i, chunk in enumerate(chunks_lib[\"small\"]):\n",
    "    fa[i] = fa[i-1] if i>0 else 0 # 初始化，因为两个chunks都是按句子切的必然有序\n",
    "    small_range = sentence_range = chunk[\"sentence_range\"]\n",
    "    large_range = chunks_lib[\"large\"][fa[i]][\"sentence_range\"]\n",
    "    while not (small_range[1]<=large_range[1] and small_range[0]>=large_range[0]) :\n",
    "        fa[i] += 1\n",
    "        large_range = chunks_lib[\"large\"][fa[i]][\"sentence_range\"]\n",
    "\n",
    "print(fa)\n",
    "\n",
    "for i, chunk in enumerate(chunks_lib[\"small\"][:3]):\n",
    "    print (f\"Chunk #{i}\")\n",
    "    print (chunk[\"text\"].replace(\"\\n\\n\",\" \").strip())\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"什么地区的人皮肤容易长痘？\",\"吸烟对身体健康的危害\",\"肤色与地区的关系\"]\n",
    "q_embeddings = model.encode_queries(queries)\n",
    "c_embeddings = {chunksize:model.encode([x[\"text\"] for x in chunks]) for chunksize,chunks in chunks_lib.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从小片段中检索\n",
    "matched_samll_chunks_ids = find_emb_matched_chunks_ids(q_embeddings, c_embeddings[\"small\"])  \n",
    "# 后处理\n",
    "min_gain = 0.05 # 取父chunk的最小提升条件\n",
    "for q_i, q_embedding in enumerate(q_embeddings):\n",
    "    print(\"\\nQuestion:\",queries[q_i])\n",
    "    return_ids = {\"large\":set(),\"small\":[]}\n",
    "    for c_i in matched_samll_chunks_ids[q_i]:\n",
    "        score = cosine_similarity(q_embedding, c_embeddings[\"small\"][c_i])\n",
    "        fa_score = cosine_similarity(q_embedding, c_embeddings[\"large\"][fa[c_i]])\n",
    "        if fa_score > score+min_gain:\n",
    "            return_ids[\"large\"].add(fa[c_i])\n",
    "    return_ids[\"small\"] = [c_i for c_i in matched_samll_chunks_ids[q_i] if fa[c_i] not in return_ids[\"large\"]]\n",
    "    print(matched_samll_chunks_ids[q_i])\n",
    "    print(return_ids)\n",
    "\n",
    "    ans = []\n",
    "    for chunk_size,ids in return_ids.items():\n",
    "        for id in ids:\n",
    "            start_sent_id, last_sent_id = chunks_lib[chunk_size][id][\"sentence_range\"]\n",
    "            start_idx, end_idx = sentences[start_sent_id][\"start_idx\"], sentences[last_sent_id][\"start_idx\"]+sentences[last_sent_id][\"length\"]\n",
    "            print(text[start_idx:end_idx].replace(\"\\n\\n\",\"\").strip())\n",
    "            print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
