{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入情绪分析任务所用数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_Dataset_of_financial_news_sentiment_classification(from_dir, to_dir):\n",
    "    # https://github.com/wwwxmu/Dataset-of-financial-news-sentiment-classification\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    train_data = pd.read_csv(f\"{from_dir}/train_data.csv\")\n",
    "    test_data = pd.read_csv(f\"{from_dir}/test_data.csv\")\n",
    "    get_text = lambda x: x['正文']\n",
    "    get_label = lambda x: x['正负面']\n",
    "    # 合并数据集\n",
    "    df = pd.concat([\n",
    "        train_data.assign(split='train', text=get_text, label=get_label),\n",
    "        test_data.assign(split='test', text=get_text, label=get_label)\n",
    "    ], ignore_index=True)[['split', 'text', 'label']]\n",
    "    # 清洗\n",
    "    df = df[df['text'].notna() & (df['text'] != '')]\n",
    "    # 保存清洗后的数据\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df.to_json(os.path.join(to_dir,'Dataset-of-financial-news-sentiment-classification.jsonl'), orient='records',force_ascii=False, lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过qlib获取股票市场数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_qlib_stock_dataset(from_dir, to_dir):\n",
    "    # https://github.com/chenditc/investment_data\n",
    "    import qlib\n",
    "    import os\n",
    "    from qlib.data import D\n",
    "    # 初始化 Qlib 的数据存储\n",
    "    qlib.init(provider_uri = from_dir)\n",
    "    fields = ['$open', '$high', '$low', '$close', '$volume', '$amount', '$vwap']\n",
    "    df = D.features(D.instruments(market='csi300'), fields, start_time='20160101', end_time='20201231', freq='day')\n",
    "    df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df.to_csv(os.path.join(to_dir,\"csi300_stock_feats.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入新闻摘要任务相关数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_FinCUGE(from_dir, to_dir):\n",
    "    # https://huggingface.co/datasets/Maciel/FinCUGE-Instruction\n",
    "    from datasets import load_dataset, concatenate_datasets\n",
    "    import os\n",
    "    dataset = load_dataset(from_dir)  \n",
    "    for split_name in dataset:\n",
    "        dataset[split_name] = dataset[split_name].map(lambda example: {\"split\": split_name})\n",
    "    combined_data = concatenate_datasets([dataset[split_name] for split_name in dataset])\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df = combined_data.to_pandas()\n",
    "    df.to_json(os.path.join(to_dir,\"FinCUGE.jsonl\"), orient='records',force_ascii=False, lines=True)\n",
    "\n",
    "def extract_finna_in_FinCUGE_as_sharegpt(from_dir, to_dir):\n",
    "    import pandas as pd  \n",
    "    import os\n",
    "    df = pd.read_json(f\"{from_dir}/FinCUGE.jsonl\", lines=True)\n",
    "    df['messages'] = df.apply(lambda row: [\n",
    "                                dict(role='system',content=row['instruction']),\n",
    "                                dict(role='user',content=row['input']),\n",
    "                                dict(role='assistant',content=row['output'])\n",
    "                            ], axis=1)\n",
    "    df[(df['split'] == 'train') & (df['task'] == 'FINNA')][['messages']].to_json(os.path.join(to_dir,\"FinCUGE_FINNA_train.jsonl\"), orient='records',force_ascii=False, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入文档主题分割评测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_CPTS(from_dir, to_dir):\n",
    "    # https://github.com/fjiangAI/CPTS\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    def boundary_to_mass(boundary_list):\n",
    "        bound_idx = [-1] + [i for i, val in enumerate(boundary_list) if val==1]\n",
    "        return [ bound_idx[i+1] - bound_idx[i] for i in range(len(bound_idx) - 1) ]\n",
    "    train_data = pd.read_json(f\"{from_dir}/train.json\")\n",
    "    test_data = pd.read_json(f\"{from_dir}/test.json\")\n",
    "    df = pd.concat([train_data.assign(split='train'),test_data.assign(split='test')], ignore_index=True) # 合并数据集\n",
    "    df['sentences'] = df.apply(lambda row: [i['text'] for i in row['paragraph_list']], axis=1)\n",
    "    df['masses'] = df['label_list'].apply(boundary_to_mass)\n",
    "    # 保存\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    df[['id','title','topic_list','sentences', 'masses','split','label_list']].to_json(os.path.join(to_dir,'CPTS.jsonl'), orient='records',force_ascii=False, lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入RAG评测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_FinanceRAG(from_dir, to_dir, llm_config, spliter_config):\n",
    "    # https://huggingface.co/datasets/Linq-AI-Research/FinanceRAG\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    from functools import partial\n",
    "    import sys; sys.path.append(\"..\")\n",
    "    from utils.spliters import init_spliter\n",
    "    from minirag.utils import compute_mdhash_id\n",
    "\n",
    "\n",
    "    client = OpenAI(base_url=llm_config['base_url'],api_key=llm_config[\"api_key\"])\n",
    "    gen_resp = partial(client.chat.completions.create,model=llm_config['model'],temperature=0.1, top_p=1, max_tokens=1000,)\n",
    "\n",
    "    ## translate_text：翻译指定文本\n",
    "    def translate_text(text):\n",
    "        response = gen_resp(messages=[dict(role=\"system\",content=\"将文字翻译成中文，直接输出翻译结果\"),dict(role=\"user\",content=text)])\n",
    "        return response.choices[0].message.content  \n",
    "\n",
    "    ## summarize_text：概括指定文本\n",
    "    def summarize_text(text):\n",
    "        response = gen_resp(messages=[dict(role=\"system\",content=\"为以下内容生成摘要，直接输出结果\"),dict(role=\"user\",content=text)])\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    query_df = pd.read_json(os.path.join(from_dir,\"queries.jsonl\"),lines=True)\n",
    "    corpus_df = pd.read_json(os.path.join(from_dir,\"corpus.jsonl\"),lines=True)\n",
    "\n",
    "    query_df[\"text_zh\"] = query_df[\"text\"].apply(translate_text)\n",
    "    query_df.to_json(os.path.join(to_dir, \"queries.json\"), orient=\"records\", indent=4, force_ascii=False)\n",
    "    corpus_df[\"text_zh\"] = corpus_df[\"text\"].apply(translate_text)\n",
    "    corpus_df.to_json(os.path.join(to_dir, \"corpus.json\"), orient=\"records\", indent=4, force_ascii=False)\n",
    "\n",
    "    spliter = init_spliter(**spliter_config)\n",
    "    chunk_sum_list = []\n",
    "    for idx, row in corpus_df.iterrows():\n",
    "        sentence_df = spliter.split_text_to_sentences(row['text_zh'])\n",
    "        sentence_df = spliter.add_buffered_sentences(sentence_df)\n",
    "        chunk_df = spliter.cluster(sentence_df)\n",
    "        for cidx, crow in chunk_df.iterrows():\n",
    "            summary = crow['chunk'] if len(crow['chunk'])<50 else summarize_text(crow['chunk'])\n",
    "            chunk_sum_list.append({\n",
    "                \"doc_id\": row[\"_id\"],\n",
    "                \"chunk_id\": compute_mdhash_id(summary.strip(), prefix=\"chunk-\"),\n",
    "                \"doc_text\": row['text_zh'],\n",
    "                \"chunk_text\": crow['chunk'],\n",
    "                \"chunk_sum_text\": summary,\n",
    "            })\n",
    "    chunk_sum_df = pd.DataFrame(chunk_sum_list)\n",
    "    chunk_sum_df.to_json(os.path.join(to_dir, \"chunk_sum.json\"), orient='records', index=False, force_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "from_dir = \"resources/data/_raw/FinanceRAG_finqa_bench\"\n",
    "to_dir = \"resources/data/finqa\"\n",
    "llm_config = dict(model=\"judger\", base_url=\"http://localhost:12235/v1\",api_key=\"empty\")\n",
    "spliter_config = dict(method=\"doc_seq_model_spliter\", model_path=\"resources/open_models/nlp_bert_document-segmentation_chinese-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为demo准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_certain_day_related_news(market_data_path, news_dir, day):\n",
    "    df_market = pd.read_excel(market_data_path)  # 读取 Excel\n",
    "    df_market['date'] = pd.to_datetime(df_market['datetime']).dt.date  # 统一日期格式\n",
    "\n",
    "    # 读取新闻数据\n",
    "    df_news = pd.concat([\n",
    "        pd.read_json(os.path.join(news_dir, filename))\n",
    "        .assign(instrument=filename.split('.')[0])\n",
    "        for filename in os.listdir(news_dir) if filename.endswith(\".json\")\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 统一新闻数据的日期格式\n",
    "    df_news['date'] = pd.to_datetime(df_news['date']).dt.date\n",
    "\n",
    "    # 找到市场数据中当天有数据的股票\n",
    "    stocks_on_day = df_market.loc[df_market['date'] == pd.to_datetime(day).date(), 'instrument'].unique()\n",
    "\n",
    "    # 筛选新闻数据，要求日期为 day，且 instrument 在 stocks_on_day 中\n",
    "    df_filtered_news = df_news[(df_news['date'] == pd.to_datetime(day).date()) & (df_news['instrument'].astype(int).isin(stocks_on_day))]\n",
    "        \n",
    "    return df_filtered_news\n",
    "\n",
    "\n",
    "market_data_path = \"demo/2019Q4股票预测.xlsx\"\n",
    "news_dir = \"resources/data/CSI300news_chunked_summarized_senti\"\n",
    "day = \"2019-10-08\"\n",
    "df_filtered_news = extract_certain_day_related_news(market_data_path, news_dir, day)\n",
    "df_filtered_news.to_json(f\"demo/{day}_相关新闻片段.json\", orient=\"records\",indent=4,index=False,force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271/271 [03:34<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "def add_stock_name(df, stock_code_col):\n",
    "    def get_stock_name(stock_code):\n",
    "        url = \"https://push2.eastmoney.com/api/qt/stock/get\"\n",
    "        params = {\n",
    "            \"secid\": f\"{'1.' if stock_code.startswith('6') else '0.'}{stock_code}\",  # 6开头是沪市，其他是深市\n",
    "            \"fields\": \"f58\"  # 只获取股票名称\n",
    "        }\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json().get(\"data\", {})\n",
    "            return data.get(\"f58\", \"股票代码不存在\").replace(\" \", \"\")\n",
    "        except requests.RequestException as e:\n",
    "            return f\"查询失败: {e}\"\n",
    "    stock_name_mapper = {instrument: get_stock_name(instrument) for instrument in tqdm(df[stock_code_col].unique())}\n",
    "    df[\"name\"] = df.apply(lambda row: stock_name_mapper[row[stock_code_col]],axis=1)\n",
    "    return df\n",
    "\n",
    "market_data_path = \"demo/2019Q4股票预测.xlsx\"\n",
    "df_market_data = pd.read_excel(market_data_path,index_col=0,dtype={\"instrument\":str})\n",
    "df_market_data = add_stock_name(df_market_data,\"instrument\")\n",
    "df_market_data.to_excel(market_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from minirag.prompt import PROMPTS\n",
    "from minirag import MiniRAG\n",
    "from minirag.utils import EmbeddingFunc, compute_mdhash_id\n",
    "from minirag.llm import openai_complete_if_cache, hf_embedding\n",
    "\n",
    "import sys; sys.path.append(\"../..\")\n",
    "from utils.rag import prompts\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nest_asyncio; nest_asyncio.apply() # 在notebook中使用async所需\n",
    "\n",
    "def build_index(embed_model_path,rag_root,rag_llm_args, series):\n",
    "\n",
    "    PROMPTS.update(prompts) \n",
    "\n",
    "    # 设置日志级别\n",
    "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "    embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_path, model_max_length=512) \n",
    "    embed_model = AutoModel.from_pretrained(embed_model_path)\n",
    "\n",
    "    os.makedirs(os.path.join(rag_root,\"rag_data\"),exist_ok=True)\n",
    "    rag = MiniRAG(\n",
    "        working_dir=os.path.join(rag_root,\"rag_data\"),\n",
    "        llm_model_func=lambda prompt,**kwargs: openai_complete_if_cache(prompt=prompt,**rag_llm_args, **kwargs,), \n",
    "        llm_model_max_token_size=1000, \n",
    "        llm_model_name=rag_llm_args[\"model\"],\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=embed_model.config.hidden_size,\n",
    "            max_token_size=embed_model.config.max_position_embeddings,\n",
    "            func=partial(hf_embedding, embed_model=embed_model, tokenizer=embed_tokenizer)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for idx, value in series.items():\n",
    "        rag.insert(value)\n",
    "\n",
    "rag_llm_args = dict(model=\"base\", base_url=\"http://localhost:12239/v1\",api_key=\"empty\")\n",
    "embed_model_path = \"resources/open_models/bge-large-zh-v1.5\" \n",
    "rag_root = \"demo/\" \n",
    "news_chunk_path = \"demo/2019-10-08_相关新闻片段.json\"\n",
    "df = pd.read_json(news_chunk_path)\n",
    "series = df.apply(lambda row: f\"标题：{row['title']}，摘要：{row['summary']}\",axis=1)\n",
    "# build_index(embed_model_path,rag_root,rag_llm_args,series)\n",
    "df['chunk_id'] = series.apply(lambda x : compute_mdhash_id(x.strip(), prefix=\"chunk-\"))\n",
    "df.to_json(news_chunk_path, index=False, force_ascii=False, indent=4,orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG评测数据--消融"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_FinanceBench(from_dir, to_dir, llm_config):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    from functools import partial\n",
    "    import sys; sys.path.append(\"..\")\n",
    "    from utils.spliters import init_spliter\n",
    "    from minirag.utils import compute_mdhash_id\n",
    "\n",
    "    client = OpenAI(base_url=llm_config['base_url'],api_key=llm_config[\"api_key\"])\n",
    "    gen_resp = partial(client.chat.completions.create,model=llm_config['model'],temperature=0.1, top_p=1, max_tokens=1000,)\n",
    "\n",
    "    ## translate_text：翻译指定文本\n",
    "    def translate_text(text):\n",
    "        response = gen_resp(messages=[dict(role=\"system\",content=\"将文字翻译成中文，直接输出翻译结果\"),dict(role=\"user\",content=text)])\n",
    "        return response.choices[0].message.content  \n",
    "    \n",
    "    query_df = pd.read_json(os.path.join(from_dir,\"queries.jsonl\"),lines=True)\n",
    "    corpus_df = pd.read_json(os.path.join(from_dir,\"corpus.jsonl\"),lines=True)\n",
    "\n",
    "    query_df[\"text_zh\"] = query_df[\"text\"].apply(translate_text)\n",
    "    query_df.to_json(os.path.join(to_dir, \"queries.json\"), orient=\"records\", indent=4, force_ascii=False)\n",
    "    corpus_df[\"text_zh\"] = corpus_df[\"text\"].apply(translate_text)\n",
    "    corpus_df.to_json(os.path.join(to_dir, \"corpus.json\"), orient=\"records\", indent=4, force_ascii=False)\n",
    "    \n",
    "\n",
    "def custom_FinanceBench(from_dir, to_dir, llm_config, spliter_config):\n",
    "    # https://huggingface.co/datasets/Linq-AI-Research/FinanceRAG\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    from functools import partial\n",
    "    import sys; sys.path.append(\"..\")\n",
    "    from utils.spliters import init_spliter\n",
    "    from minirag.utils import compute_mdhash_id\n",
    "\n",
    "    client = OpenAI(base_url=llm_config['base_url'],api_key=llm_config[\"api_key\"])\n",
    "    gen_resp = partial(client.chat.completions.create,model=llm_config['model'],temperature=0.1, top_p=1, max_tokens=1000,)\n",
    "\n",
    "    ## summarize_text：概括指定文本\n",
    "    def summarize_text(text):\n",
    "        response = gen_resp(messages=[dict(role=\"system\",content=\"为以下内容生成摘要，直接输出结果\"),dict(role=\"user\",content=text)])\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    query_df = pd.read_json(os.path.join(from_dir,\"queries.jsonl\"),lines=True)\n",
    "    corpus_df = pd.read_json(os.path.join(from_dir,\"corpus.jsonl\"),lines=True)\n",
    "\n",
    "    spliter = init_spliter(**spliter_config)\n",
    "    chunk_sum_list = []\n",
    "    for idx, row in corpus_df.iterrows():\n",
    "        sentence_df = spliter.split_text_to_sentences(row['text_zh'])\n",
    "        sentence_df = spliter.add_buffered_sentences(sentence_df)\n",
    "        chunk_df = spliter.cluster(sentence_df)\n",
    "        for cidx, crow in chunk_df.iterrows():\n",
    "            summary = crow['chunk'] if len(crow['chunk'])<50 else summarize_text(crow['chunk'])\n",
    "            chunk_sum_list.append({\n",
    "                \"doc_id\": row[\"_id\"],\n",
    "                \"chunk_id\": compute_mdhash_id(summary.strip(), prefix=\"chunk-\"),\n",
    "                \"doc_text\": row['text_zh'],\n",
    "                \"chunk_text\": crow['chunk'],\n",
    "                \"chunk_sum_text\": summary,\n",
    "            })\n",
    "    chunk_sum_df = pd.DataFrame(chunk_sum_list)\n",
    "    chunk_sum_df.to_json(os.path.join(to_dir, \"chunk_sum.json\"), orient='records', index=False, force_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = dict(model=\"judger\", base_url=\"http://localhost:12235/v1\",api_key=\"empty\")\n",
    "translate_FinanceBench(\"resources/data/_raw/FinanceBench\",\"resources/data/ablation_rag/FinanceBench/zh\",llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_config = dict(model=\"judger\", base_url=\"http://localhost:12235/v1\",api_key=\"empty\")\n",
    "spliter_config = dict(method=\"doc_seq_model_spliter\", model_path=\"resources/open_models/nlp_bert_document-segmentation_chinese-base\")\n",
    "custom_FinanceRAG(\"resources/data/ablation_rag/FinanceBench/zh\",\"resources/data/ablation_rag/FinanceBench/\",)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
